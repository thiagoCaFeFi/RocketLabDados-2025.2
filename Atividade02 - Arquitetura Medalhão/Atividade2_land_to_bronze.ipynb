{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dee0c84-cf91-47cb-bef6-a9ffef44f513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "# Configurações\n",
    "catalogo = \"medalhao\"\n",
    "bronze_db_name = \"bronze\"\n",
    "landing_schema_name = \"default\"\n",
    "landing_volume_name = \"landing\"\n",
    "\n",
    "# Caminho base para os arquivos CSV na Landing\n",
    "landing_path = f\"/Volumes/{catalogo}/{landing_schema_name}/{landing_volume_name}\"\n",
    "\n",
    "# Garantir que o catálogo e o schema bronze existam\n",
    "spark.sql(f\"USE CATALOG {catalogo}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_db_name}\")\n",
    "\n",
    "print(f\"Contexto configurado para Catálogo: {catalogo}, Schema: {bronze_db_name}\")\n",
    "print(f\"Lendo arquivos da Landing em: {landing_path}\")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db53b636-e0a3-49bb-a0ad-60c751ec24af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão de Arquivos\n",
    "arquivos_csv = {\n",
    "    \"olist_customers_dataset.csv\": \"ft_consumidores\",\n",
    "    \"olist_geolocation_dataset.csv\": \"ft_geolocalizacao\",\n",
    "    \"olist_order_items_dataset.csv\": \"ft_itens_pedidos\",\n",
    "    \"olist_order_payments_dataset.csv\": \"ft_pagamentos_pedidos\",\n",
    "    \"olist_order_reviews_dataset.csv\": \"ft_avaliacoes_pedidos\",\n",
    "    \"olist_orders_dataset.csv\": \"ft_pedidos\",\n",
    "    \"olist_products_dataset.csv\": \"ft_produtos\",\n",
    "    \"olist_sellers_dataset.csv\": \"ft_vendedores\",\n",
    "    \"product_category_name_translation.csv\": \"dim_categoria_produtos_traducao\"\n",
    "}\n",
    "\n",
    "for nome_arquivo, nome_tabela in arquivos_csv.items():\n",
    "    try:\n",
    "        print(f\"➡️ Processando: {nome_arquivo} → {bronze_db_name}.{nome_tabela}\")\n",
    "        \n",
    "        caminho_arquivo = f\"{landing_path}/{nome_arquivo}\"\n",
    "        \n",
    "        df = (\n",
    "            spark.read\n",
    "            .csv(caminho_arquivo, header=True, inferSchema=True)\n",
    "            .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        )\n",
    "        \n",
    "        # --- CORREÇÃO ---\n",
    "        # Salva diretamente como Tabela Gerenciada no schema 'bronze'\n",
    "        # Usamos o nome completo (catalogo.schema.tabela) para garantir\n",
    "        nome_completo_tabela = f\"{catalogo}.{bronze_db_name}.{nome_tabela}\"\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(nome_completo_tabela)\n",
    "        \n",
    "        displayHTML(f\"<span style='color:limegreen; font-weight:bold;'> Tabela CSV criada com sucesso:</span> {nome_completo_tabela}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        displayHTML(f\"<span style='color:red; font-weight:bold;'> Erro ao criar tabela CSV:</span> {nome_completo_tabela}<br><b>Detalhes:</b> {str(e)}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Ingestão de CSVs concluída.\")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81cbd9eb-ae6d-46b7-86b0-008c8e20c40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão da Cotação do Dólar\n",
    "data_inicio_formatada = \"11-03-2025\" \n",
    "data_fim_formatada = \"11-07-2025\"   \n",
    "\n",
    "# Tabela destino\n",
    "tabela_dolar = \"dm_cotacao_dolar\"\n",
    "nome_completo_tabela_dolar = f\"{catalogo}.{bronze_db_name}.{tabela_dolar}\"\n",
    "\n",
    "url = (\n",
    "    \"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata/\"\n",
    "    \"CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)?\"\n",
    "    f\"@dataInicial='{data_inicio_formatada}'&@dataFinalCotacao='{data_fim_formatada}'\"\n",
    "    \"&$select=dataHoraCotacao,cotacaoCompra&$format=json\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()[\"value\"]\n",
    "        \n",
    "        if not data:\n",
    "             displayHTML(f\"<h3 style='color:orange;'> API não retornou dados para o período.</h3>\"\n",
    "                         f\"<p>Verifique as datas: {data_inicio_formatada} a {data_fim_formatada}</p>\")\n",
    "        else:\n",
    "            # Criação do DataFrame\n",
    "            df_dolar = spark.createDataFrame(data)\n",
    "            df_dolar = df_dolar.withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "            \n",
    "            # Salva diretamente como Tabela Gerenciada\n",
    "            df_dolar.write.format(\"delta\").mode(\"overwrite\").saveAsTable(nome_completo_tabela_dolar)\n",
    "            \n",
    "            # Informações de sucesso\n",
    "            qtd_registros = df_dolar.count()\n",
    "            ultima_data = df_dolar.agg(F.max(\"dataHoraCotacao\")).first()[0]\n",
    "            \n",
    "            displayHTML(f\"\"\"\n",
    "            <h3 style='color:limegreen;'> Cotação do Dólar carregada com sucesso!</h3>\n",
    "            <ul>\n",
    "                <li><b>Tabela:</b> {nome_completo_tabela_dolar}</li>\n",
    "                <li><b>Período:</b> {data_inicio_formatada} → {data_fim_formatada}</li>\n",
    "                <li><b>Registros carregados:</b> {qtd_registros}</li>\n",
    "                <li><b>Última data de cotação:</b> {ultima_data}</li>\n",
    "            </ul>\n",
    "            \"\"\")\n",
    "    \n",
    "    else:\n",
    "        displayHTML(f\"\"\"\n",
    "        <h3 style='color:red;'> Erro ao consultar API do Banco Central</h3>\n",
    "        <p>Status HTTP: {response.status_code}</p>\n",
    "        <p>URL: {url}</p>\n",
    "        <p>Verifique a URL ou as datas informadas.</p>\n",
    "        \"\"\")\n",
    "\n",
    "except Exception as e:\n",
    "     displayHTML(f\"<span style='color:red; font-weight:bold;'> Erro ao criar tabela de Cotação:</span> {nome_completo_tabela_dolar}<br><b>Detalhes:</b> {str(e)}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Processo de ingestão para a camada Bronze concluído.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Atividade2_land_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
